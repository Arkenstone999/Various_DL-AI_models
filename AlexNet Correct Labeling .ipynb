{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac348998-a2af-45c0-9c66-b30c0053d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import copy\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f7c1c82-0a86-414b-835c-2b83c276dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('combined_df.pkl')\n",
    "df = df.dropna(subset=['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d8253d5-47e0-4f96-876f-268c22dd4984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engagement_type\n",
      "engaged-negative    13160\n",
      "engaged-positive    12874\n",
      "not engaged          3652\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "engagement_mapping = {\n",
    "    \"not engaged\": [\n",
    "        \"isolation\", \"neglect\", \"pity\", \"sentimentality\", \"loneliness\", \"gloom\", \"alienation\", \"defeat\", \"anguish\", \"dejection\",\n",
    "        \"hopelessness\", \"melancholy\", \"depression\", \"homesickness\", \"longing\"\n",
    "    ],\n",
    "    \"engaged-positive\": [\n",
    "        \"lust\", \"desire\", \"infatuation\", \"passion\", \"attraction\", \"liking\",\n",
    "        \"excitement\", \"hope\", \"optimism\", \"eagerness\", \"zeal\", \"arousal\", \"joy\", \"zest\",\n",
    "        \"cheerfulness\", \"happiness\", \"elation\", \"rapture\", \"enjoyment\", \"gladness\",\n",
    "        \"bliss\", \"gaiety\", \"jubilation\", \"delight\", \"euphoria\", \"jolliness\", \"joviality\",\n",
    "        \"glee\", \"ecstasy\", \"caring\", \"love\", \"tenderness\", \"affection\", \"adoration\",\n",
    "        \"fondness\", \"compassion\", \"sympathy\", \"pleasure\", \"pride\", \"satisfaction\",\n",
    "        \"contentment\", \"relief\", \"triumph\", \"enthusiasm\", \"amusement\", \"surprise\",\n",
    "        \"astonishment\", \"amazement\", \"shock\", \"thrill\", \"exhilaration\", \"enthrallment\"\n",
    "    ],\n",
    "    \"engaged-negative\": [\n",
    "        \"irritation\", \"wrath\", \"annoyance\", \"rage\", \"aggravation\", \"anger\", \"resentment\",\n",
    "        \"grumpiness\", \"frustration\", \"fury\", \"hostility\", \"exasperation\", \"outrage\",\n",
    "        \"grouchiness\", \"spite\", \"unhappiness\", \"disappointment\", \"insult\",\n",
    "        \"rejection\", \"agitation\", \"bitterness\", \"hate\",\n",
    "        \"disgust\", \"dislike\", \"contempt\", \"scorn\", \"displeasure\", \"envy\", \"loathing\",\n",
    "        \"jealousy\", \"revulsion\", \"nervousness\", \"alarm\", \"fear\", \"fright\", \"horror\",\n",
    "        \"terror\", \"dread\", \"hysteria\", \"dismay\", \"apprehension\", \"worry\", \"panic\",\n",
    "        \"tenseness\", \"uneasiness\", \"anxiety\", \"suffering\", \"hurt\", \"agony\",\n",
    "        \"insecurity\", \"distress\", \"torment\", \"sadness\", \"grief\", \"glumness\", \"sorrow\", \"despair\", \"misery\", \"woe\", \"regret\", \"guilt\", \"shame\", \"embarrassment\", \"mortification\",\n",
    "        \"remorse\", \"humiliation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 1: Flatten the mapping\n",
    "flat_mapping = {}\n",
    "for engagement_type, labels in engagement_mapping.items():\n",
    "    for label in labels:\n",
    "        flat_mapping[label] = engagement_type\n",
    "\n",
    "# Step 2: Apply mapping to your DataFrame\n",
    "df['engagement_type'] = df['label'].map(flat_mapping)\n",
    "\n",
    "# Step 3: Optional - check distribution\n",
    "print(df['engagement_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eefda8c-7987-4652-9206-530ad8f221a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'description', 'label', 'base_name', 'emotion_category',\n",
       "       'image', 'engagement_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d995730a-f9fa-44b9-9a50-15b87d25b8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "      <th>base_name</th>\n",
       "      <th>emotion_category</th>\n",
       "      <th>engagement_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigstockphoto_irritation_portrait_450_147.jpg</td>\n",
       "      <td>Image of displeased beautiful woman talking on...</td>\n",
       "      <td>irritation</td>\n",
       "      <td>bigstockphoto_irritation_portrait_450_147</td>\n",
       "      <td>Anger / Irritation</td>\n",
       "      <td>engaged-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alamy_frustration_face_10_76.jpg</td>\n",
       "      <td>Cute little blue eyed european blond boy looks...</td>\n",
       "      <td>frustration</td>\n",
       "      <td>alamy_frustration_face_10_76</td>\n",
       "      <td>Anger / Irritation</td>\n",
       "      <td>engaged-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alamy_irritation_portrait_10_10.jpg</td>\n",
       "      <td>Portrait of young man with acne problem at home</td>\n",
       "      <td>irritation</td>\n",
       "      <td>alamy_irritation_portrait_10_10</td>\n",
       "      <td>Anger / Irritation</td>\n",
       "      <td>engaged-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shutterstock_wrath_look_10_89.jpg</td>\n",
       "      <td>Portrait Angry Business Man Fists Air Stock Ph...</td>\n",
       "      <td>wrath</td>\n",
       "      <td>shutterstock_wrath_look_10_89</td>\n",
       "      <td>Anger / Irritation</td>\n",
       "      <td>engaged-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alamy_irritation_face_11_98.jpg</td>\n",
       "      <td></td>\n",
       "      <td>irritation</td>\n",
       "      <td>alamy_irritation_face_11_98</td>\n",
       "      <td>Anger / Irritation</td>\n",
       "      <td>engaged-negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name  \\\n",
       "0  bigstockphoto_irritation_portrait_450_147.jpg   \n",
       "3               alamy_frustration_face_10_76.jpg   \n",
       "4            alamy_irritation_portrait_10_10.jpg   \n",
       "5              shutterstock_wrath_look_10_89.jpg   \n",
       "6                alamy_irritation_face_11_98.jpg   \n",
       "\n",
       "                                         description        label  \\\n",
       "0  Image of displeased beautiful woman talking on...   irritation   \n",
       "3  Cute little blue eyed european blond boy looks...  frustration   \n",
       "4    Portrait of young man with acne problem at home   irritation   \n",
       "5  Portrait Angry Business Man Fists Air Stock Ph...        wrath   \n",
       "6                                                      irritation   \n",
       "\n",
       "                                   base_name    emotion_category  \\\n",
       "0  bigstockphoto_irritation_portrait_450_147  Anger / Irritation   \n",
       "3               alamy_frustration_face_10_76  Anger / Irritation   \n",
       "4            alamy_irritation_portrait_10_10  Anger / Irritation   \n",
       "5              shutterstock_wrath_look_10_89  Anger / Irritation   \n",
       "6                alamy_irritation_face_11_98  Anger / Irritation   \n",
       "\n",
       "    engagement_type  \n",
       "0  engaged-negative  \n",
       "3  engaged-negative  \n",
       "4  engaged-negative  \n",
       "5  engaged-negative  \n",
       "6  engaged-negative  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['name', 'description', 'label', 'base_name', 'emotion_category', 'engagement_type']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8063fec-1094-48d2-a273-506a8c1d74b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engagement_type\n",
      "engaged-negative    3652\n",
      "engaged-positive    3652\n",
      "not engaged         3652\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\3667488993.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the target sample size\n",
    "min_class_size = df['engagement_type'].value_counts().min()\n",
    "\n",
    "# Step 2: Sample each class down to the minimum size\n",
    "df_balanced = (\n",
    "    df.groupby('engagement_type', group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Step 3: Check the balance\n",
    "print(df_balanced['engagement_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18a667c5-b119-4cc9-97a5-941e512ee2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['engagement_label_encoder.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df_balanced['label_encoded'] = le.fit_transform(df_balanced['engagement_type'])\n",
    "joblib.dump(le, 'engagement_label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40d26006-aa1c-4adf-982e-086fe2829037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngagementDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.images = dataframe['image'].tolist()\n",
    "        self.labels = dataframe['label_encoded'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d07b4467-3f57-4699-b347-fc06173bd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_balanced, test_size=0.2, stratify=df_balanced['label_encoded'], random_state=42)\n",
    "\n",
    "train_dataset = EngagementDataset(train_df)\n",
    "val_dataset = EngagementDataset(val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9eaaa3e-d63b-41cf-96a8-fc928c0fa2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(le.classes_)\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9606b0a7-da2e-4504-849e-6713c407d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db45b4da-6e43-4e61-bc5f-fea197c111c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\3431682102.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n",
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\3431682102.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.9789 | Val Accuracy: 56.71%\n",
      "Epoch 2/10 | Train Loss: 0.8616 | Val Accuracy: 57.44%\n",
      "Epoch 3/10 | Train Loss: 0.7700 | Val Accuracy: 57.71%\n",
      "Epoch 4/10 | Train Loss: 0.6615 | Val Accuracy: 56.98%\n",
      "Epoch 5/10 | Train Loss: 0.5427 | Val Accuracy: 58.67%\n",
      "Epoch 6/10 | Train Loss: 0.4289 | Val Accuracy: 57.39%\n",
      "Epoch 7/10 | Train Loss: 0.3264 | Val Accuracy: 57.12%\n",
      "Epoch 8/10 | Train Loss: 0.2538 | Val Accuracy: 57.12%\n",
      "Epoch 9/10 | Train Loss: 0.2169 | Val Accuracy: 56.89%\n",
      "Epoch 10/10 | Train Loss: 0.1713 | Val Accuracy: 57.25%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- TRAINING --------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # -------- LOG RESULTS --------\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3908fea-d5aa-4fc4-8fe8-a295678c0127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\4156458488.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n",
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\4156458488.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.1557 | Train Acc: 94.12% | Val Loss: 1.6248 | Val Acc: 57.80%\n",
      "Epoch 2/10 | Train Loss: 0.1415 | Train Acc: 94.69% | Val Loss: 1.6574 | Val Acc: 58.49%\n",
      "Epoch 3/10 | Train Loss: 0.1329 | Train Acc: 94.60% | Val Loss: 1.8441 | Val Acc: 56.98%\n",
      "Epoch 4/10 | Train Loss: 0.1171 | Train Acc: 95.32% | Val Loss: 1.6399 | Val Acc: 56.89%\n",
      "Epoch 5/10 | Train Loss: 0.1106 | Train Acc: 95.46% | Val Loss: 1.9287 | Val Acc: 55.93%\n",
      "Epoch 6/10 | Train Loss: 0.1100 | Train Acc: 95.20% | Val Loss: 1.9009 | Val Acc: 57.03%\n",
      "Epoch 7/10 | Train Loss: 0.1127 | Train Acc: 95.25% | Val Loss: 1.8682 | Val Acc: 59.35%\n",
      "‚úÖ New best model saved.\n",
      "Epoch 8/10 | Train Loss: 0.1002 | Train Acc: 95.60% | Val Loss: 2.0391 | Val Acc: 58.53%\n",
      "Epoch 9/10 | Train Loss: 0.1028 | Train Acc: 95.94% | Val Loss: 2.0941 | Val Acc: 54.84%\n",
      "Epoch 10/10 | Train Loss: 0.0924 | Train Acc: 95.77% | Val Loss: 2.0452 | Val Acc: 58.85%\n",
      "\n",
      "üèÅ Best validation accuracy: 59.35%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "num_epochs = 10\n",
    "#best_val_accuracy = 0.0\n",
    "#best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- TRAINING --------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # -------- LOG RESULTS --------\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # -------- SAVE BEST MODEL --------\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(\"‚úÖ New best model saved.\")\n",
    "\n",
    "# -------- LOAD BEST MODEL AFTER TRAINING --------\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nüèÅ Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå No improvement during training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a42253b-d2a5-44ea-b642-e8d553ba1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af195d-b119-4a3a-a240-aa2488a0acee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "179a035a-4b98-42d9-9d76-94884dd5b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\4156458488.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n",
      "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_19128\\4156458488.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, labels = images.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.1089 | Train Acc: 95.61% | Val Loss: 2.3149 | Val Acc: 57.39%\n",
      "Epoch 2/10 | Train Loss: 0.1006 | Train Acc: 95.85% | Val Loss: 2.4829 | Val Acc: 58.03%\n",
      "Epoch 3/10 | Train Loss: 0.1032 | Train Acc: 95.45% | Val Loss: 2.0431 | Val Acc: 58.30%\n",
      "Epoch 4/10 | Train Loss: 0.0969 | Train Acc: 95.92% | Val Loss: 2.3320 | Val Acc: 56.89%\n",
      "Epoch 5/10 | Train Loss: 0.0936 | Train Acc: 95.70% | Val Loss: 2.6946 | Val Acc: 57.76%\n",
      "Epoch 6/10 | Train Loss: 0.0928 | Train Acc: 95.96% | Val Loss: 2.1022 | Val Acc: 58.03%\n",
      "Epoch 7/10 | Train Loss: 0.0857 | Train Acc: 96.04% | Val Loss: 2.4626 | Val Acc: 58.26%\n",
      "Epoch 8/10 | Train Loss: 0.0811 | Train Acc: 96.50% | Val Loss: 2.0467 | Val Acc: 56.30%\n",
      "Epoch 9/10 | Train Loss: 0.0833 | Train Acc: 96.25% | Val Loss: 2.4335 | Val Acc: 58.44%\n",
      "Epoch 10/10 | Train Loss: 0.0755 | Train Acc: 96.33% | Val Loss: 2.1407 | Val Acc: 57.85%\n",
      "\n",
      "üèÅ Best validation accuracy: 59.35%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "num_epochs = 10\n",
    "#best_val_accuracy = 0.0\n",
    "#best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- TRAINING --------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), torch.tensor(labels).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # -------- LOG RESULTS --------\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # -------- SAVE BEST MODEL --------\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(\"‚úÖ New best model saved.\")\n",
    "\n",
    "# -------- LOAD BEST MODEL AFTER TRAINING --------\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nüèÅ Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå No improvement during training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b784f8e-078e-419a-ba5c-6c36b07f42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"alexnet_best_final_corrected.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d339f546-6e43-4d9b-b7d9-b3ac51e8f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b98d1-147b-4531-bf0b-aabb2e3f1193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb48fc-71e2-4286-9b81-21d5636ac1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a66dcc-d375-4511-9b30-027cd8dfcd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdc453-f868-45e3-ab3e-680b08c83c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fca2f-5279-4a70-b52e-81a8d05fb181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73fee369-330b-4be0-8cf3-5c05c4d00823",
   "metadata": {},
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.6),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.6),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes)\n",
    ")\n",
    "\n",
    "model.to(device)  # üîÅ Move full model to the correct device AFTER modifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb4311-8efa-4e13-a815-c91125585100",
   "metadata": {},
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "num_epochs = 30\n",
    "best_val_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- TRAINING --------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val, total_val = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # -------- SAVE BEST MODEL --------\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = copy.deepcopy(model.state_dict())  # Save model weights\n",
    "        print(f\"‚úÖ New best model found at epoch {epoch+1} with Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # -------- LOG RESULTS --------\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# -------- LOAD BEST MODEL AFTER TRAINING --------\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"üèÅ Best validation accuracy was: {best_val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6b598-810e-431c-bfdb-9685daa8f84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c95b3-b7ad-4c12-a28f-f5cdb5eeee80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb03f0-b86d-4e4d-b34d-4bd5bbb6c879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b465f9a-7754-4085-959b-976fd7dd982a",
   "metadata": {},
   "source": [
    "# TESTING THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268ec71d-c94b-4e04-9ce5-90e9799159b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Webcam started. Press 'q' to quit.\n",
      "\n",
      "‚èπÔ∏è Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torchvision import transforms\n",
    "import joblib\n",
    "\n",
    "# ===== Load model (if not already) =====\n",
    "#model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# ===== Load label encoder =====\n",
    "le = joblib.load(\"engagement_label_encoder.joblib\")\n",
    "\n",
    "# ===== Define preprocessing =====\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ===== Open webcam =====\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot open webcam.\")\n",
    "else:\n",
    "    print(\"üì∑ Webcam started. Press 'q' to quit.\")\n",
    "\n",
    "last_prediction_time = 0\n",
    "prediction_interval = 2  # seconds\n",
    "current_prediction = \"...\"\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ùå Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Only run prediction every 2 seconds\n",
    "        if time.time() - last_prediction_time >= prediction_interval:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = preprocess(rgb_frame).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                pred_idx = output.argmax(dim=1).item()\n",
    "                current_prediction = le.inverse_transform([pred_idx])[0]\n",
    "\n",
    "            last_prediction_time = time.time()\n",
    "\n",
    "        # Draw prediction on frame\n",
    "        cv2.putText(frame, f'Engagement: {current_prediction}', (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show webcam feed with overlay\n",
    "        cv2.imshow(\"Live Engagement Detection\", frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Stopped by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093c1e1-327e-4c21-a90a-24d5a5d07de4",
   "metadata": {},
   "source": [
    "## Debuging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9db8917-caa5-4053-a1ea-1fbc7f036a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not engaged'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([pred_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e17c034-37e7-4517-ac05-9f8e6cc86ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['engaged-negative' 'engaged-positive' 'not engaged']\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe550ec-a387-4c24-964e-e03855d9beff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32eccf6d-e1db-4b37-97c2-6d58330bee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engagement_type\n",
      "engaged-negative    3652\n",
      "engaged-positive    3652\n",
      "not engaged         3652\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced['engagement_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb81cd8-697d-470b-9ef9-1ee6405df43b",
   "metadata": {},
   "source": [
    "All three classes are perfectly balanced at 3652 samples each, so overfitting due to class imbalance is not the issue. This means the model had a fair chance to learn all engagement types, but still defaults to predicting \"not engaged\" at test time. So now we move to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8a59d-ada6-4ce1-8a03-489a124e6182",
   "metadata": {},
   "source": [
    "#### Test the model on a known sample from validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91009626-e961-4947-9c75-924281fef1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ True label:      engaged-negative\n",
      "ü§ñ Model predicted: engaged-negative\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Pick a known validation sample ‚Äî change index if needed\n",
    "image, true_label = val_dataset[0]\n",
    "\n",
    "# Prepare input\n",
    "input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "# Run prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_idx = output.argmax(dim=1).item()\n",
    "\n",
    "# Decode predicted and true labels\n",
    "predicted_label = le.inverse_transform([pred_idx])[0]\n",
    "true_label_name = le.inverse_transform([true_label])[0]\n",
    "\n",
    "print(f\"‚úÖ True label:      {true_label_name}\")\n",
    "print(f\"ü§ñ Model predicted: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215d86a-d1e2-4487-89a8-492396ed35f5",
   "metadata": {},
   "source": [
    "Your model correctly predicted engaged-negative for a known validation sample ‚Äî this confirms that:\n",
    "\n",
    "üß† The model itself is working\n",
    "\n",
    "‚úÖ Inference decoding is correct\n",
    "\n",
    "üß™ Training and validation pipelines are solid\n",
    "\n",
    "So the problem is not with the model, but likely in the webcam input pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880809bc-0e77-4f96-a4d5-6a6ec75184ef",
   "metadata": {},
   "source": [
    "#### Debug the webcam input & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74a5686-d4ae-4af7-b221-97797394c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to grab frame.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Capture one frame\n",
    "ret, frame = cap.read()\n",
    "cap.release()  # Release after capture\n",
    "\n",
    "if not ret:\n",
    "    print(\"‚ùå Failed to grab frame.\")\n",
    "else:\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess\n",
    "    input_tensor = preprocess(rgb_frame)\n",
    "\n",
    "    # De-normalize for display\n",
    "    unnorm = transforms.Normalize(\n",
    "        mean=[-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "        std=[1/s for s in [0.229, 0.224, 0.225]]\n",
    "    )\n",
    "    image_vis = unnorm(input_tensor).permute(1, 2, 0).cpu().numpy()\n",
    "    image_vis = (image_vis * 255).clip(0, 255).astype('uint8')\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.imshow(image_vis)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Preprocessed Image Sent to Model\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0306d-de8e-4266-9047-76ed8ec40c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
